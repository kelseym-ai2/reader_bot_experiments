{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adccc87e-e16f-48ea-bed4-c04164728b30",
   "metadata": {},
   "source": [
    "SDS (MPE)\n",
    "\n",
    "All the sourced papers associated to a PDF hash:\n",
    "https://www.google.com/url?sa=j&url=http%3A%2F%2Fsds.dev.s2.allenai.org%2Fv1%2FSourcedPapers%3Fpdf%3Db1bee7d90b3aa30a8bc6c9a2a9f2995030dc3b2c%26limit%3D10&uct=1673549986&usg=g53p8pHmbKntQ3CIqU5w2uaYDm0.&source=meet\n",
    "\n",
    "Content of PDF:\n",
    "http://sds.dev.s2.allenai.org/v1/SourcedPapers?pdf=b1bee7d90b3aa30a8bc6c9a2a9f2995030dc3b2c&limit=10&elements=Sections\n",
    "\n",
    "Note: could make a second call to sh2owable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617fb636-5e82-4697-b6a1-8f5d12fb14d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "042b93dd-7726-4379-acbc-19413287f8c5",
   "metadata": {},
   "source": [
    "Notes from Expandable sumamries demo:\n",
    "- Q/A\n",
    "    - embedded chunks (~256 words)\n",
    "    - embedded query\n",
    "    - match query to chunks\n",
    "    - concatenate chunks, that becomes the context. prompt with question and context.\n",
    "    - GPT-4 could pass the full paper (up to 5k words, could limit feature to papers under this length)\n",
    "    - write prompt that says something like \"using only the evidence\"\n",
    "    - temperature setting to control abstractiveness (vs extractiveness)\n",
    "    - can pre-write questions\n",
    "- Inserting in context (\"fluid\")\n",
    "    - expand on term in context\n",
    "    - or give examples\n",
    "    - prompt \"tell me more\" about `term` in context \n",
    "- precompute the terms\n",
    "    - ask GPT-4 to create questions from an abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81566a-1881-42af-8e77-41cf00f0943a",
   "metadata": {},
   "source": [
    "Script for generate finetuning pairs: https://github.com/allenai/term-understanding/blob/main/scripts/create_pair_gpt3_finetune_dataset.py\n",
    "\n",
    "Open AI API docs: https://platform.openai.com/docs/introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa2caa-b456-46be-862b-59f41a4a2aa7",
   "metadata": {
    "tags": []
   },
   "source": [
    "PDF chatbot tools:\n",
    "https://github.com/mayooear/gpt4-pdf-chatbot-langchain\n",
    "\n",
    "some usable demos: https://pdfgpt.io/ and https://www.chatpdf.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84fa2979-f139-4924-9492-c2b301302727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a scientific knowledge assistant. A user has given you the following \\\n",
    "\n",
    "prompt: \"{{ prompt }}\"\n",
    "\n",
    " \n",
    "\n",
    "Using a search engine, you have found the following papers:\n",
    "\n",
    "{% for paper in state.stack %}\n",
    "\n",
    "[{{ paper.id }}] {{ paper.title }}\n",
    "\n",
    "{{ paper.abstract }}\n",
    "\n",
    "{% endfor %}\\\n",
    "\n",
    " \n",
    "\n",
    "Write a response to the user that answers their information need. Make sure \\\n",
    "\n",
    "to only use information from the previous results. When writing a claim, \\\n",
    "\n",
    "cite the relevant paper(s) using the format `[n]` (for example, `[1]` for \\\n",
    "\n",
    "the first paper). The list of results above is not exhaustive, so you need to \\\n",
    "\n",
    "make sure that the user knows that.\n",
    "\n",
    " \n",
    "\n",
    "Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56c0d49c-4161-4238-bb7f-3f20a36d31a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a scientific knowledge assistant. A user has given you the following \n",
      "prompt: \"{{ prompt }}\"\n",
      "\n",
      " \n",
      "\n",
      "Using a search engine, you have found the following papers:\n",
      "\n",
      "{% for paper in state.stack %}\n",
      "\n",
      "[{{ paper.id }}] {{ paper.title }}\n",
      "\n",
      "{{ paper.abstract }}\n",
      "\n",
      "{% endfor %}\n",
      " \n",
      "\n",
      "Write a response to the user that answers their information need. Make sure \n",
      "to only use information from the previous results. When writing a claim, \n",
      "cite the relevant paper(s) using the format `[n]` (for example, `[1]` for \n",
      "the first paper). The list of results above is not exhaustive, so you need to \n",
      "make sure that the user knows that.\n",
      "\n",
      " \n",
      "\n",
      "Response:\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010834e-4f23-4a2d-ad5a-d6f223e0033a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
